# 2.1 Probability

Course: Neural Networks, Deep Learning (https://www.notion.so/Neural-Networks-Deep-Learning-2b50e07166a6483fbfc11ccbdedbc202?pvs=21)
Confidence: Not Confident
Last Edited: June 4, 2024 5:16 PM
Is Completed: No

# Probability

**åŸºæœ¬æ¦‚å¿µï¼š**

- **æ ·æœ¬ç©ºé—´ï¼ˆSample Spaceï¼‰**ï¼šæ ·æœ¬ç©ºé—´ `â„¦` æ˜¯**æ‰€æœ‰å¯èƒ½ç»“æœ**çš„é›†åˆ
- **æ ·æœ¬ç‚¹ï¼ˆSample Pointï¼‰**ï¼šæ ·æœ¬ç©ºé—´ä¸­çš„æ¯ä¸ªå…ƒç´  `Ï‰`
- **æ¦‚ç‡ç©ºé—´ï¼ˆProbability Spaceï¼‰**ï¼šæ¦‚ç‡ç©ºé—´æˆ–æ¦‚ç‡æ¨¡å‹æ˜¯æŒ‡ä¸€ä¸ªæ ·æœ¬ç©ºé—´ â„¦ï¼Œå¹¶ä¸”å¯¹äº â„¦ ä¸­çš„æ¯ä¸ªæ ·æœ¬ç‚¹ Ï‰ï¼Œéƒ½æœ‰ä¸€ä¸ªæ¦‚ç‡å€¼ P(Ï‰)
    - éè´Ÿæ€§
    - æ‰€æœ‰æ ·æœ¬ç‚¹çš„æ¦‚ç‡ä¹‹å’Œç­‰äº 1

# **Gaussian Distributions**

ä¸€ä¸ªéšæœºå˜é‡ `x` æœä»å‡å€¼ä¸º `*Î¼`* ä¸”æ ‡å‡†å·®ä¸º `ğœ` çš„æ­£æ€åˆ†å¸ƒï¼Œæ¦‚ç‡å¯†åº¦å‡½æ•°ä¸ºï¼š

$$
P_{\mu,\sigma}(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

- `*Î¼*` æ˜¯å‡å€¼ï¼ˆmeanï¼‰
- `*Ïƒ*` æ˜¯æ ‡å‡†å·®ï¼ˆstandard deviationï¼‰ï¼Œè¡¨ç¤ºæ•°æ®çš„åˆ†æ•£ç¨‹åº¦
- `*Ïƒ^*2` æ˜¯æ–¹å·®ï¼ˆvarianceï¼‰ï¼Œç­‰äºæ ‡å‡†å·®çš„å¹³æ–¹

**Gaussian Distributionsç‰¹æ€§:**

- çº¦ `68%` çš„æ•°æ®è½åœ¨å‡å€¼ `*Î¼*` ä¸¤ä¾§ä¸€ä¸ªæ ‡å‡†å·® `*Ïƒ*` ä»¥å†…ã€‚
- çº¦ `95%` çš„æ•°æ®è½åœ¨å‡å€¼ `*Î¼*` ä¸¤ä¾§ä¸¤ä¸ªæ ‡å‡†å·® `2*Ïƒ*` ä»¥å†…ã€‚
- çº¦ `99.7%` çš„æ•°æ®è½åœ¨å‡å€¼ `*Î¼*` ä¸¤ä¾§ä¸‰ä¸ªæ ‡å‡†å·® `3*Ïƒ*` ä»¥å†…

ä¸Šé¢çš„æ˜¯åœ¨ä¸€ç»´ç©ºé—´ä¸­ `x` çš„é«˜æ–¯åˆ†å¸ƒï¼Œä¸‹é¢è¦ä»‹ç»çš„æ˜¯åœ¨å¤šç»´ç©ºé—´ä¸­çš„é«˜æ–¯åˆ†å¸ƒ

## Multivariate Gaussians **Distributions**

å¯¹äºä¸€ä¸ª `ğ‘‘ç»´` çš„å¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼Œå‡å€¼å‘é‡ä¸º `ğœ‡`ï¼ˆç»´åº¦ä¸º `ğ‘‘` çš„å‘é‡ï¼‰ï¼Œåæ–¹å·®çŸ©é˜µä¸º `Î£`ï¼ˆ `*d*Ã—*d`* çš„çŸ©é˜µï¼‰ï¼Œå…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸ºï¼š

$$
P_{\mu,\Sigma}(x) = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} e^{-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)}
$$

- `*x`* æ˜¯ `ğ‘‘` ç»´çš„éšæœºå‘é‡
- `*Î¼`* æ˜¯ `ğ‘‘` ç»´çš„å‡å€¼å‘é‡
- `Î£` æ˜¯ `ğ‘‘Ã—ğ‘‘` çš„åæ–¹å·®çŸ©é˜µ
- `âˆ£Î£âˆ£` è¡¨ç¤ºåæ–¹å·®çŸ©é˜µ `Î£` çš„è¡Œåˆ—å¼
- `Î£^{âˆ’1}` è¡¨ç¤ºåæ–¹å·®çŸ©é˜µ `Î£` çš„é€†çŸ©é˜µ

**ç‰¹æ®Šæƒ…å†µï¼šå¯¹è§’åæ–¹å·®çŸ©é˜µ:**

å¦‚æœåæ–¹å·®çŸ©é˜µ `Î£` æ˜¯å¯¹è§’çŸ©é˜µ(å…¶éå¯¹è§’çº¿å…ƒç´ éƒ½ä¸º `0`)ï¼Œé‚£ä¹ˆMultivariate Gaussians Distributionså¯ä»¥ç®€åŒ–ä¸ºå¦‚ä¸‹ï¼š

$$
P_{\mu,\Sigma}(x) = \prod_{i=1}^{n} P_{\mu_i, \sigma_i}(x_i)
$$

å½“å‡å€¼å‘é‡ `ğœ‡=0`ï¼Œåæ–¹å·®çŸ©é˜µ`Î£=ğ¼`ï¼ˆå•ä½çŸ©é˜µï¼‰æ—¶ï¼Œå¤šå…ƒé«˜æ–¯åˆ†å¸ƒç§°ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼ˆStandard Normal Distributionï¼‰ï¼Œå…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸ºï¼š

$$
P_{\mathbf{0},\mathbf{I}}(\mathbf{x}) = \frac{1}{(2\pi)^{d/2}} \exp\left(-\frac{1}{2} \mathbf{x}^T \mathbf{x}\right)
$$

# Conditional Probability

æ¡ä»¶æ¦‚ç‡æŒ‡çš„æ˜¯åœ¨å·²çŸ¥æŸäº›ä¿¡æ¯æˆ–äº‹ä»¶å‘ç”Ÿçš„å‰æä¸‹,å¦ä¸€ä¸ªäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ `P(A|B)` ï¼Œ`B` å·²ç»å‘ç”Ÿ

$$
P(A \mid B) = \frac{P(A \land B)}{P(B)}
$$

## Bayes' Rule

æ ¹æ®*ä¹˜æ³•æ³•åˆ™*ï¼ˆmultiplication rule ï¼‰å¯å¾—åˆ° $ğ‘ƒ(ğ´,ğµ)=ğ‘ƒ(ğµâˆ£ğ´)ğ‘ƒ(ğ´)$ã€‚æ ¹æ®å¯¹ç§°æ€§ï¼Œå¯å¾—åˆ° $ğ‘ƒ(ğ´,ğµ)=ğ‘ƒ(Aâˆ£B)ğ‘ƒ(B)$ã€‚å‡è®¾ $ğ‘ƒ(ğµ)>0$ï¼Œæ±‚è§£å…¶ä¸­ä¸€ä¸ªæ¡ä»¶å˜é‡ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š

$$
P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}
$$

Bayes' Ruleçš„æ„ä¹‰ï¼šåˆ©ç”¨æ–°çš„è¯æ®æˆ–ä¿¡æ¯æ¥æ›´æ–°å·²çŸ¥çš„å…ˆéªŒæ¦‚ç‡,ä»è€Œè·å¾—åéªŒæ¦‚ç‡ã€‚æä¾›äº†ä¸€ç§ç³»ç»Ÿçš„æ–¹æ³•æ¥æ›´æ–°æ¦‚ç‡ä¼°è®¡

## Entropy and KL-Divergence

ç”¨äºé‡åŒ–æ¦‚ç‡åˆ†å¸ƒä¸­çš„ä¸ç¡®å®šæ€§(Entropy)å’Œå·®å¼‚(KL-Divergence)

### Entropy(ç†µ)

åº¦é‡äº†ä¸€ä¸ª**ç¦»æ•£æ¦‚ç‡**åˆ†å¸ƒä¸­çš„ä¸ç¡®å®šæ€§ç¨‹åº¦ï¼Œè®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š

$$
H(p) = -\sum_{i=1}^{n} p_i \log_{2} p_i
$$

- `*p_i`* è¡¨ç¤ºäº‹ä»¶ `*i`* å‘ç”Ÿçš„æ¦‚ç‡
- `log_{2}*p_i`* æ˜¯ä»¥æ¯”ç‰¹ä¸ºå•ä½åº¦é‡ä¿¡æ¯é‡

ç†µè¶Šé«˜ï¼Œè¡¨ç¤ºåˆ†å¸ƒä¸­çš„ä¸ç¡®å®šæ€§è¶Šå¤§

### **KL-Divergence(ç›¸å¯¹ç†µ)**

ç”¨äºè¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œè®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š

$$
DKL(p \| q) = \sum_{i=1}^{n} p_i (\log_2 p_i - \log_2 q_i)
$$

- `*p_i`* æ˜¯äº‹ä»¶ `ğ‘–` åœ¨åˆ†å¸ƒ `*p*`ä¸­çš„æ¦‚ç‡
- `*q_i`* æ˜¯äº‹ä»¶ `ğ‘–` åœ¨åˆ†å¸ƒ qä¸­çš„æ¦‚ç‡

**KL-Divergenceçš„æ€§è´¨ï¼š**

- **éå¯¹ç§°æ€§ï¼š**$D_{KL}(p \| q) \neq D_{KL}(q \| p)$
- **éè´Ÿæ€§ï¼š**åªæœ‰å½“ `*p`* å’Œ `*q`* å®Œå…¨ç›¸åŒæ—¶ï¼ŒKL-Divergenceæ‰ä¸ºé›¶

ä¸Šé¢çš„Entropy and KL-Divergenceåº¦é‡çš„éƒ½æ˜¯ç¦»æ•£æ¦‚ç‡åˆ†å¸ƒä¸‹çš„ï¼Œåœ¨å¤„ç†**è¿ç»­åˆ†å¸ƒ**æ—¶ï¼Œæˆ‘ä»¬ä¹Ÿæœ‰ç±»ä¼¼äºç¦»æ•£åˆ†å¸ƒçš„ç†µå’ŒKLæ•£åº¦çš„æ¦‚å¿µï¼Œä½†è¡¨ç¤ºæ–¹å¼ä¼š**ç”¨åˆ°ç§¯åˆ†**è€Œä¸æ˜¯æ±‚å’Œ

### The Entropy of a Continuous Distribution

$$
H(p) = \int_{\Theta} p(\theta) \left( -\log p(\theta) \right) \, d\theta
$$

### KL-Divergence between Two Continuous Distributions

$$
D_{KL}(p \| q) = \int_{\Theta} p(\theta) \left( \log p(\theta) - \log q(\theta) \right) \, d\theta
$$

å¤„ç†KL-Divergenceï¼Œè¿˜æœ‰ä¸€ç§è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚çš„æ–¹æ³•ï¼Œå°±æ˜¯**Wasserstein Distance**

### Entropy for Gaussian Distributions

**Entropy of Gaussian with mean Âµ and standard deviation Ïƒ :**

$$
\frac{1}{2} \left(1 + \log\left(2\pi\right)\right) + \log(\sigma)
$$

**Entropy of a d-dimensional Gaussian p() with mean Âµ and variance Î£:**

$$
H(p) = \frac{d}{2} \left(1 + \log(2\pi)\right) + \frac{1}{2} \log | \Sigma |
$$

**If Î£ = diag(Ïƒ12, . . . , Ïƒd2) is diagonal, the entropy is:**

$$
H(p) = \frac{d}{2} \left(1 + \log(2\pi)\right) + \sum_{i=1}^{d} \log(\sigma_i)
$$

### KL-Divergence for Gaussians

**KL-Divergence between Gaussians q(), p() with mean Âµ1, Âµ2 and variance Î£1, Î£2:**

$$
D_{KL}(q \| p) = \frac{1}{2} \left( \mu_2 - \mu_1 \right)^T \Sigma^{-1} \left( \mu_2 - \mu_1 \right) + \frac{1}{2} \text{Tr}\left( \Sigma^{-1} \Sigma_1 \right) + \log \left\| \Sigma \Sigma_2^{-1} \right\| - \frac{d}{2}
$$

**In the case where Âµ2 = 0, Î£2 = I, the KL-Divergence simplifies to:**

$$
D_{KL}(q \| p) = \frac{1}{2} \left( \|\mu_1\|^2 + \text{Tr}(\Sigma_1) - \log |\Sigma_1| - d \right)
$$

**If Î£1 = diag(Ïƒ12, . . . , Ïƒd2) is diagonal, this reduces to:**

$$
D_{KL}(q \| p) = \frac{1}{2} \|\mu_1\|^2 + \sum_{i=1}^{d} \left( \frac{\sigma_i^2}{2} - \log(\sigma_i) - \frac{1}{2} \right)
$$

## **Wasserstein Distance**

# å‚è€ƒæ–‡ç« 

[Dive into deep learning](https://zh.d2l.ai/chapter_preliminaries/probability.html#id5)