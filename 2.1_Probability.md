# 2.1 Probability

Course: Neural Networks, Deep Learning (https://www.notion.so/Neural-Networks-Deep-Learning-2b50e07166a6483fbfc11ccbdedbc202?pvs=21)
Confidence: Not Confident
Last Edited: June 4, 2024 5:16 PM
Is Completed: No

# Probability

**基本概念：**

- **样本空间（Sample Space）**：样本空间 `Ω` 是**所有可能结果**的集合
- **样本点（Sample Point）**：样本空间中的每个元素 `ω`
- **概率空间（Probability Space）**：概率空间或概率模型是指一个样本空间 Ω，并且对于 Ω 中的每个样本点 ω，都有一个概率值 P(ω)
    - 非负性
    - 所有样本点的概率之和等于 1

# **Gaussian Distributions**

一个随机变量 `x` 服从均值为 `*μ`* 且标准差为 `𝜎` 的正态分布，概率密度函数为：

$$
P_{\mu,\sigma}(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

- `*μ*` 是均值（mean）
- `*σ*` 是标准差（standard deviation），表示数据的分散程度
- `*σ^*2` 是方差（variance），等于标准差的平方

**Gaussian Distributions特性:**

- 约 `68%` 的数据落在均值 `*μ*` 两侧一个标准差 `*σ*` 以内。
- 约 `95%` 的数据落在均值 `*μ*` 两侧两个标准差 `2*σ*` 以内。
- 约 `99.7%` 的数据落在均值 `*μ*` 两侧三个标准差 `3*σ*` 以内

上面的是在一维空间中 `x` 的高斯分布，下面要介绍的是在多维空间中的高斯分布

## Multivariate Gaussians **Distributions**

对于一个 `𝑑维` 的多元高斯分布，均值向量为 `𝜇`（维度为 `𝑑` 的向量），协方差矩阵为 `Σ`（ `*d*×*d`* 的矩阵），其概率密度函数为：

$$
P_{\mu,\Sigma}(x) = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} e^{-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)}
$$

- `*x`* 是 `𝑑` 维的随机向量
- `*μ`* 是 `𝑑` 维的均值向量
- `Σ` 是 `𝑑×𝑑` 的协方差矩阵
- `∣Σ∣` 表示协方差矩阵 `Σ` 的行列式
- `Σ^{−1}` 表示协方差矩阵 `Σ` 的逆矩阵

**特殊情况：对角协方差矩阵:**

如果协方差矩阵 `Σ` 是对角矩阵(其非对角线元素都为 `0`)，那么Multivariate Gaussians Distributions可以简化为如下：

$$
P_{\mu,\Sigma}(x) = \prod_{i=1}^{n} P_{\mu_i, \sigma_i}(x_i)
$$

当均值向量 `𝜇=0`，协方差矩阵`Σ=𝐼`（单位矩阵）时，多元高斯分布称为标准正态分布（Standard Normal Distribution），其概率密度函数为：

$$
P_{\mathbf{0},\mathbf{I}}(\mathbf{x}) = \frac{1}{(2\pi)^{d/2}} \exp\left(-\frac{1}{2} \mathbf{x}^T \mathbf{x}\right)
$$

# Conditional Probability

条件概率指的是在已知某些信息或事件发生的前提下,另一个事件发生的概率 `P(A|B)` ，`B` 已经发生

$$
P(A \mid B) = \frac{P(A \land B)}{P(B)}
$$

## Bayes' Rule

根据*乘法法则*（multiplication rule ）可得到 $𝑃(𝐴,𝐵)=𝑃(𝐵∣𝐴)𝑃(𝐴)$。根据对称性，可得到 $𝑃(𝐴,𝐵)=𝑃(A∣B)𝑃(B)$。假设 $𝑃(𝐵)>0$，求解其中一个条件变量，我们得到：

$$
P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}
$$

Bayes' Rule的意义：利用新的证据或信息来更新已知的先验概率,从而获得后验概率。提供了一种系统的方法来更新概率估计

## Entropy and KL-Divergence

用于量化概率分布中的不确定性(Entropy)和差异(KL-Divergence)

### Entropy(熵)

度量了一个**离散概率**分布中的不确定性程度，计算公式如下：

$$
H(p) = -\sum_{i=1}^{n} p_i \log_{2} p_i
$$

- `*p_i`* 表示事件 `*i`* 发生的概率
- `log_{2}*p_i`* 是以比特为单位度量信息量

熵越高，表示分布中的不确定性越大

### **KL-Divergence(相对熵)**

用于衡量两个概率分布之间的差异，计算公式如下：

$$
DKL(p \| q) = \sum_{i=1}^{n} p_i (\log_2 p_i - \log_2 q_i)
$$

- `*p_i`* 是事件 `𝑖` 在分布 `*p*`中的概率
- `*q_i`* 是事件 `𝑖` 在分布 q中的概率

**KL-Divergence的性质：**

- **非对称性：**$D_{KL}(p \| q) \neq D_{KL}(q \| p)$
- **非负性：**只有当 `*p`* 和 `*q`* 完全相同时，KL-Divergence才为零

上面的Entropy and KL-Divergence度量的都是离散概率分布下的，在处理**连续分布**时，我们也有类似于离散分布的熵和KL散度的概念，但表示方式会**用到积分**而不是求和

### The Entropy of a Continuous Distribution

$$
H(p) = \int_{\Theta} p(\theta) \left( -\log p(\theta) \right) \, d\theta
$$

### KL-Divergence between Two Continuous Distributions

$$
D_{KL}(p \| q) = \int_{\Theta} p(\theta) \left( \log p(\theta) - \log q(\theta) \right) \, d\theta
$$

处理KL-Divergence，还有一种衡量两个概率分布之间的差异的方法，就是**Wasserstein Distance**

### Entropy for Gaussian Distributions

**Entropy of Gaussian with mean µ and standard deviation σ :**

$$
\frac{1}{2} \left(1 + \log\left(2\pi\right)\right) + \log(\sigma)
$$

**Entropy of a d-dimensional Gaussian p() with mean µ and variance Σ:**

$$
H(p) = \frac{d}{2} \left(1 + \log(2\pi)\right) + \frac{1}{2} \log | \Sigma |
$$

**If Σ = diag(σ12, . . . , σd2) is diagonal, the entropy is:**

$$
H(p) = \frac{d}{2} \left(1 + \log(2\pi)\right) + \sum_{i=1}^{d} \log(\sigma_i)
$$

### KL-Divergence for Gaussians

**KL-Divergence between Gaussians q(), p() with mean µ1, µ2 and variance Σ1, Σ2:**

$$
D_{KL}(q \| p) = \frac{1}{2} \left( \mu_2 - \mu_1 \right)^T \Sigma^{-1} \left( \mu_2 - \mu_1 \right) + \frac{1}{2} \text{Tr}\left( \Sigma^{-1} \Sigma_1 \right) + \log \left\| \Sigma \Sigma_2^{-1} \right\| - \frac{d}{2}
$$

**In the case where µ2 = 0, Σ2 = I, the KL-Divergence simplifies to:**

$$
D_{KL}(q \| p) = \frac{1}{2} \left( \|\mu_1\|^2 + \text{Tr}(\Sigma_1) - \log |\Sigma_1| - d \right)
$$

**If Σ1 = diag(σ12, . . . , σd2) is diagonal, this reduces to:**

$$
D_{KL}(q \| p) = \frac{1}{2} \|\mu_1\|^2 + \sum_{i=1}^{d} \left( \frac{\sigma_i^2}{2} - \log(\sigma_i) - \frac{1}{2} \right)
$$

## **Wasserstein Distance**

# 参考文章

[Dive into deep learning](https://zh.d2l.ai/chapter_preliminaries/probability.html#id5)